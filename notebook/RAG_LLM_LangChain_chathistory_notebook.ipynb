{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d237da1e",
   "metadata": {},
   "source": [
    "## RAG LLM by LangChain - contextual chat with chat history \n",
    "\n",
    "Document: SageMaker FAQ   \n",
    "Models: huggingface embedding and mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764d1b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98992ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9afacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f17da",
   "metadata": {},
   "source": [
    "Prepare embedding model for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b6cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c916a31",
   "metadata": {},
   "source": [
    "###### Document embedding and vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae036c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents \n",
    "loader = PyPDFDirectoryLoader(\"./data/smdoc\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents by setting chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,     \n",
    "    chunk_overlap = 50,    \n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "805c9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store, it will take some time ...  \n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)\n",
    "\n",
    "# Save store to local disk \n",
    "vectorstore_faiss.save_local(\"smdoc_vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ad8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vector store from local disk\n",
    "vectorstore_faiss = FAISS.load_local(\"smdoc_vectordb\", embeddings)\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1596aa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Test doc search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bba21e4-e6dc-492a-b535-9210597fd67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "----\n",
      "## Document 1: commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing\n",
      "Calculator.\n",
      "Q: How can I optimize my SageMaker costs, such as detecting and stopping idle resources to\n",
      "avoid unnecessary charges?\n",
      "There are several best practices that you can adopt to optimize your SageMaker resource usage.\n",
      "Some approaches involve conﬁguration optimizations; others involve programmatic solutions. A.......\n",
      "---\n",
      "## Document 2: inference workloads at any time and automatically continue to pay the Savings Plans price.\n",
      "Q: Why should I use SageMaker Savings Plans?\n",
      "If you have a consistent amount of SageMaker instance usage (measured in $/hour) and use\n",
      "multiple SageMaker components or expect your technology conﬁguration (such as instance family,\n",
      "or Region) to change over time, SageMaker Savings Plans make it simpler to maximize your savings.......\n",
      "---\n",
      "## Document 3: guide for using SageMaker Studio notebooks.\n",
      "Q: How do I monitor and shut down the resources used by my notebooks?\n",
      "You can monitor and shut down the resources used by your SageMaker Studio notebooks through\n",
      "both SageMaker Studio visual interface and the AWS Management Console. See the documentation\n",
      "for more details.\n",
      "Q: I’m running a SageMaker Studio notebook. Will I still be charged if I close my browser, close\n",
      "the notebook tab, or just leave the browser open? \n",
      "Get Started for Free Contact Us.......\n",
      "---\n",
      "## Document 4: such scenarios is not cost-eﬀective, as you end up paying for idle periods. SageMaker Serverless\n",
      "Inference helps address these types of use cases by providing you automatic and fast scaling out of\n",
      "the box without the need for you to forecast traﬃc up front or manage scaling policies.\n",
      "Additionally, you pay only for the compute time to run your inference code (billed in milliseconds)\n",
      "and for data processing, making it a cost-eﬀective option for workloads with intermittent traﬃc........\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I optimize my SageMaker costs, such as detecting and stopping idle resources avoid unnecessary charges? \"\n",
    "\n",
    "query_embedding = vectorstore_faiss.embedding_function(query)\n",
    "\n",
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57def6f7",
   "metadata": {},
   "source": [
    "##### Setup LLM in RAG - Mistral-7bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9de983-82a0-40d4-b2a8-452074d65351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ccab5f-0719-4079-9c36-c3a3aa842bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c3afe99-8092-415a-a6f2-26071d4d37b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(  \n",
    "    BASE_MODEL,  \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5e9728-d7e7-430f-ab0a-a41de5a0fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 25 19:37:46 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\n",
      "| N/A   32C    P0    59W / 300W |   9839MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\n",
      "| N/A   32C    P0    60W / 300W |   9897MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\n",
      "| N/A   32C    P0    63W / 300W |   9897MiB / 16384MiB |      7%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   34C    P0    64W / 300W |   8763MiB / 16384MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     5104MiB |\n",
      "|    0   N/A  N/A     42602      C   ...s/pytorch_p310/bin/python     4732MiB |\n",
      "|    1   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     5142MiB |\n",
      "|    1   N/A  N/A     42602      C   ...s/pytorch_p310/bin/python     4752MiB |\n",
      "|    2   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     5142MiB |\n",
      "|    2   N/A  N/A     42602      C   ...s/pytorch_p310/bin/python     4752MiB |\n",
      "|    3   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     4638MiB |\n",
      "|    3   N/A  N/A     42602      C   ...s/pytorch_p310/bin/python     4122MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b56996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline('text-generation', \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                #use_cache=True,\n",
    "                #do_sample=True,\n",
    "                ##temperature = 0.2,\n",
    "                ##top_p = 0.92,\n",
    "                #top_k=5,\n",
    "                ##max_length=1000,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=256,\n",
    "                num_return_sequences=1,\n",
    "                repetition_penalty = 1.5,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "llm_mistral = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4cf1e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The current Chief Executive Officer (CEO) of Amazon Web Services (AWS), a subsidiary of Amazon, is Andy Jassy.\n"
     ]
    }
   ],
   "source": [
    "response = llm_mistral(\"<s>[INST] Who is the CEO of AWS? [/INST]\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8114d5",
   "metadata": {},
   "source": [
    "##### Create QA Bot by LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd7d6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97938bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for Q&A \n",
    "prompt_template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Provide a concise answer based on the context.\n",
    "\n",
    "Answer the question below from context below :\n",
    "\n",
    "{context}\n",
    "\n",
    "{question}  [/INST]\"\"\"\n",
    "\n",
    "ANSWER_PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "#ANSWER_PROMPT = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2101d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=llm_mistral, prompt=ANSWER_PROMPT, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d8c6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does SageMaker secure my code?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e01e4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the 3 neighbor docs \n",
    "docs = vectorstore_faiss.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9a730bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q: What security measures does SageMaker have?\\nSageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and\\nat rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You\\npass AWS Identity and Access Management roles to SageMaker to provide permissions to access\\nresources on your behalf for training and deployment. You can use encrypted Amazon Simple', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 1}),\n",
       " Document(page_content='downtimes. SageMaker APIs run in Amazon proven high-availability data centers, with service stack\\nreplication conﬁgured across three facilities in each Region to provide fault tolerance in the event\\nof a server failure or Availability Zone outage.\\nQ: How does SageMaker secure my code?\\nSageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted\\nat rest.\\nQ: What security measures does SageMaker have?', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 1}),\n",
       " Document(page_content='Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key\\nManagement Service (AWS KMS) key to SageMaker notebooks, training jobs, and endpoints to\\nencrypt the attached ML storage volume. SageMaker also supports Amazon Virtual Private Cloud\\n(Amazon VPC) and AWS PrivateLink support.\\nQ: Does SageMaker use or share models, training data, or algorithms?\\nSageMaker does not use or share customer models, training data, or algorithms. We know that', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 1}),\n",
       " Document(page_content='Q: How do SageMaker Studio notebooks work?\\nSageMaker Studio notebooks are one-step Jupyter notebooks that can be spun quickly. The\\nunderlying compute resources are fully elastic, so you can easily dial up or down the available\\nresources and the changes take place automatically in the background without interrupting your\\nwork. SageMaker also enables one-step sharing of notebooks. You can easily share notebooks with\\nothers and they’ll get the exact same notebook, saved in the same place.', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 15}),\n",
       " Document(page_content=\"Security is the top priority at AWS, and SageMaker JumpStart is designed to be secure. That's why\\nSageMaker gives you ownership and control over your content through simpliﬁed, powerful tools\\nthat help you determine where your content will be stored, secure your content in transit and at\\nrest, and manage your access to AWS services and resources for your users.\\n1. We do not share customer training and inference information with model sellers in the AWS\", metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 6})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e63872a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SageMaker secures user code using encryption both during transmission via SSL connections and while it is being processed within its environment. Additionally, all files uploaded into SageMaker Storage Volumes used for storing code are protected by IAM Roles assigned to them which allows fine grained permission management.\n"
     ]
    }
   ],
   "source": [
    "response = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)[\n",
    "    \"output_text\"\n",
    "]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8adb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b82f37f1",
   "metadata": {},
   "source": [
    "##### Method 1: use Conversational QA Chain for chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4baa5357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7a1d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template to generate new prompt based on chat history\n",
    "\n",
    "_template = \"\"\"<s>[INST]  Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow Up Input: \n",
    "{question}\n",
    "\n",
    "Assistant: [/INST]\"\"\"\n",
    "\n",
    "#CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate(template=_template, input_variables=[\"chat_history\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "819386a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for Q&A \n",
    "template = \"\"\"<s>[INST]  You are a helpful, respectful and honest assistant. Provide a concise answer based on the context.\n",
    "\n",
    "Answer the question below from context below :\n",
    "\n",
    "{context}\n",
    "\n",
    "{question}  \n",
    "\n",
    "[/INST]\"\"\"\n",
    "\n",
    "#ANSWER_PROMPT = PromptTemplate.from_template(template)\n",
    "ANSWER_PROMPT = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc8d3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "def _combine_documents(docs, document_prompt = DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2886692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:        \n",
    "        human = \"Human: \" + dialogue_turn[0]\n",
    "        ai = \"Assistant: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4edc43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore_faiss.as_retriever(search_kwargs={'k': 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9adc23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.schema import format_document\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1c4c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inputs = RunnableMap(\n",
    "    {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: _format_chat_history(x['chat_history'])\n",
    "        } | CONDENSE_QUESTION_PROMPT | llm_mistral | StrOutputParser(),\n",
    "    }\n",
    ")\n",
    "\n",
    "_context = {\n",
    "    \"context\": itemgetter(\"standalone_question\") | retriever | _combine_documents,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a18d20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_qa_chain = _inputs | _context | ANSWER_PROMPT | llm_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "553aa4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When using Amazon SageMaker, your code is stored in ML storage volumes, secured by security\n"
     ]
    }
   ],
   "source": [
    "response = conversational_qa_chain.invoke({\n",
    "    \"question\": \"How does SageMaker secure my code?\",\n",
    "    \"chat_history\": [],\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67ff3056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " When storing code on AWS SageMaker's ML Storage Volumes, it is stored at rest using encryption options provided through Security Groups and optional encryption keys passed via AWS KMS. Additionally, requests to interact with these resources are done over SSL connections.\n"
     ]
    }
   ],
   "source": [
    "response = conversational_qa_chain.invoke({\n",
    "    \"question\": \"What security does it secure by?\",\n",
    "    \"chat_history\": [(\"How does SageMaker secure my code?\", \n",
    "                      \"When using Amazon SageMaker, your code is stored in ML storage volumes, secured by security\")],\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12ac5eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, I would be happy to help! The AWS Key Management Service (KMS) provides encryption keys that protect sensitive information stored in various AWS resources including those related to Amazon SageMaker's machine learning workflows. When working with these types of files, users must first obtain permission by passing their identity via AWS Identity & Access Management (IAM). Once granted this permission, any subsequent requests will utilize the provided KMS key to decipher the content before allowing further processing. This way, only authorized individuals who possess the necessary credentials can view or modify confidential data while ensuring its protection at both rest and during transmission across networks.\n"
     ]
    }
   ],
   "source": [
    "response = conversational_qa_chain.invoke({\n",
    "    \"question\": \"How does AWS KMS help in this case?\",\n",
    "    \"chat_history\": [(\"How does SageMaker secure my code?\", \n",
    "                      \"When using Amazon SageMaker, your code is stored in ML storage volumes, secured by security\"),\n",
    "                    (\"What security does it secure by?\",\n",
    "                     \"When storing code on AWS SageMaker's ML Storage Volumes, it is stored at rest using encryption options provided through Security Groups and optional encryption keys passed via AWS KMS. Additionally, requests to interact with these resources are done over SSL connections.\")\n",
    "                    ],\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dcfa57",
   "metadata": {},
   "source": [
    "##### Using memory to store the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aea95906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "451e2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "def _format_chat_history_memory(chat_history: List[Tuple]) -> str:\n",
    "    print(\"Chat history:\", len(chat_history)/2)\n",
    "    buffer = \"\"\n",
    "    for i in range(int(len(chat_history)/2)):\n",
    "        human = \"Human: \" + chat_history[i*2].content\n",
    "        ai = \"Assistant: \" + chat_history[i*2+1].content\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f45e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we add a step to load memory\n",
    "# This needs to be a RunnableMap because its the first input\n",
    "loaded_memory = RunnableMap(\n",
    "    {\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"memory\": memory.load_memory_variables,\n",
    "    }\n",
    ")\n",
    "# Next we add a step to expand memory into the variables\n",
    "expanded_memory = {\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "    \"chat_history\": lambda x: x[\"memory\"][\"history\"]\n",
    "}\n",
    "\n",
    "# Now we calculate the standalone question\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history_memory(x['chat_history'])\n",
    "    } | CONDENSE_QUESTION_PROMPT | llm_mistral | StrOutputParser(),\n",
    "}\n",
    "# Now we retrieve the documents\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"]\n",
    "}\n",
    "# Now we construct the inputs for the final prompt\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\")\n",
    "}\n",
    "# And finally, we do the part that returns the answers\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm_mistral,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c95f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we put it all together!\n",
    "final_chain = loaded_memory | expanded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cbbd8d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' When using Amazon SageMaker, your code is stored in ML storage volumes, secured by security'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"How does SageMaker secure my code?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9399bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the memory does not save automatically\n",
    "# This will be improved in the future\n",
    "# For now you need to save it yourself\n",
    "memory.save_context(inputs, {\"answer\": result[\"answer\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a147b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='How does SageMaker secure my code?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' When using Amazon SageMaker, your code is stored in ML storage volumes, secured by security', additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it up\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b462f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" When storing code on AWS SageMaker's ML Storage Volumes, it is stored at rest using encryption options provided through Security Groups and optional encryption keys passed via AWS KMS. Additionally, requests to interact with these resources are done over SSL connections.\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"What security does it secure by?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7b1cf78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"answer\": result[\"answer\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "390b6e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='How does SageMaker secure my code?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' When using Amazon SageMaker, your code is stored in ML storage volumes, secured by security', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What security does it secure by?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" When storing code on AWS SageMaker's ML Storage Volumes, it is stored at rest using encryption options provided through Security Groups and optional encryption keys passed via AWS KMS. Additionally, requests to interact with these resources are done over SSL connections.\", additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c54f6306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat history: 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Yes, I would be happy to help! The AWS Key Management Service (KMS) provides encryption keys that protect sensitive information stored in various AWS resources including those related to Amazon SageMaker's machine learning workflows. When working with these types of files, users must first obtain permission by passing their identity via AWS Identity & Access Management (IAM). Once granted this permission, any subsequent requests will utilize the provided KMS key to decipher the content before allowing further processing. This way, only authorized individuals who possess the necessary credentials can view or modify confidential data while ensuring its protection at both rest and during transmission across networks.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"question\": \"How does AWS KMS help in this case?\"}\n",
    "result = final_chain.invoke(inputs)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56f9630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context(inputs, {\"answer\": result[\"answer\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7bd27db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='How does SageMaker secure my code?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=' When using Amazon SageMaker, your code is stored in ML storage volumes, secured by security', additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='What security does it secure by?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" When storing code on AWS SageMaker's ML Storage Volumes, it is stored at rest using encryption options provided through Security Groups and optional encryption keys passed via AWS KMS. Additionally, requests to interact with these resources are done over SSL connections.\", additional_kwargs={}, example=False),\n",
       "  HumanMessage(content='How does AWS KMS help in this case?', additional_kwargs={}, example=False),\n",
       "  AIMessage(content=\" Yes, I would be happy to help! The AWS Key Management Service (KMS) provides encryption keys that protect sensitive information stored in various AWS resources including those related to Amazon SageMaker's machine learning workflows. When working with these types of files, users must first obtain permission by passing their identity via AWS Identity & Access Management (IAM). Once granted this permission, any subsequent requests will utilize the provided KMS key to decipher the content before allowing further processing. This way, only authorized individuals who possess the necessary credentials can view or modify confidential data while ensuring its protection at both rest and during transmission across networks.\", additional_kwargs={}, example=False)]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "12b1b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready to store the chat history in a file or db, for the future training purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c86b8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
