{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d237da1e",
   "metadata": {},
   "source": [
    "## RAG LLM by LangChain - simple Q&A bot\n",
    "\n",
    "Document: SageMaker FAQ   \n",
    "Models: huggingface embedding and mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40093c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "842693b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9afacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3963c3d",
   "metadata": {},
   "source": [
    "Prepare embedding model for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21396746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0e6ce3",
   "metadata": {},
   "source": [
    "###### Document embedding and vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a039783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents \n",
    "loader = PyPDFDirectoryLoader(\"./data/smdoc\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "# Split documents by setting chunk size\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,     \n",
    "    chunk_overlap = 50,    \n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960322eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store, it will take some time ...  \n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)\n",
    "\n",
    "# Save store to local disk \n",
    "vectorstore_faiss.save_local(\"smdoc_vectordb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ad8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vector store from local disk\n",
    "vectorstore_faiss = FAISS.load_local(\"smdoc_vectordb\", embeddings)\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1596aa9",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### Test doc search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bba21e4-e6dc-492a-b535-9210597fd67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 documents are fetched which are relevant to the query.\n",
      "----\n",
      "## Document 1: commitments. For more details, see Amazon SageMaker Pricing and the Amazon SageMaker Pricing\n",
      "Calculator.\n",
      "Q: How can I optimize my SageMaker costs, such as detecting and stopping idle resources to\n",
      "avoid unnecessary charges?\n",
      "There are several best practices that you can adopt to optimize your SageMaker resource usage.\n",
      "Some approaches involve conﬁguration optimizations; others involve programmatic solutions. A.......\n",
      "---\n",
      "## Document 2: inference workloads at any time and automatically continue to pay the Savings Plans price.\n",
      "Q: Why should I use SageMaker Savings Plans?\n",
      "If you have a consistent amount of SageMaker instance usage (measured in $/hour) and use\n",
      "multiple SageMaker components or expect your technology conﬁguration (such as instance family,\n",
      "or Region) to change over time, SageMaker Savings Plans make it simpler to maximize your savings.......\n",
      "---\n",
      "## Document 3: guide for using SageMaker Studio notebooks.\n",
      "Q: How do I monitor and shut down the resources used by my notebooks?\n",
      "You can monitor and shut down the resources used by your SageMaker Studio notebooks through\n",
      "both SageMaker Studio visual interface and the AWS Management Console. See the documentation\n",
      "for more details.\n",
      "Q: I’m running a SageMaker Studio notebook. Will I still be charged if I close my browser, close\n",
      "the notebook tab, or just leave the browser open? \n",
      "Get Started for Free Contact Us.......\n",
      "---\n",
      "## Document 4: such scenarios is not cost-eﬀective, as you end up paying for idle periods. SageMaker Serverless\n",
      "Inference helps address these types of use cases by providing you automatic and fast scaling out of\n",
      "the box without the need for you to forecast traﬃc up front or manage scaling policies.\n",
      "Additionally, you pay only for the compute time to run your inference code (billed in milliseconds)\n",
      "and for data processing, making it a cost-eﬀective option for workloads with intermittent traﬃc........\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I optimize my SageMaker costs, such as detecting and stopping idle resources avoid unnecessary charges? \"\n",
    "\n",
    "query_embedding = vectorstore_faiss.embedding_function(query)\n",
    "\n",
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d352f",
   "metadata": {},
   "source": [
    "##### Setup LLM in RAG - Mistral-7bi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a9de983-82a0-40d4-b2a8-452074d65351",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ccab5f-0719-4079-9c36-c3a3aa842bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c3afe99-8092-415a-a6f2-26071d4d37b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.55s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32000, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(  \n",
    "    BASE_MODEL,  \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5e9728-d7e7-430f-ab0a-a41de5a0fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 25 19:10:45 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1B.0 Off |                    0 |\r\n",
      "| N/A   32C    P0    59W / 300W |   4735MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:00:1C.0 Off |                    0 |\r\n",
      "| N/A   32C    P0    61W / 300W |   4755MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   2  Tesla V100-SXM2...  On   | 00000000:00:1D.0 Off |                    0 |\r\n",
      "| N/A   32C    P0    64W / 300W |   4755MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   33C    P0    64W / 300W |   4125MiB / 16384MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     4732MiB |\r\n",
      "|    1   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     4752MiB |\r\n",
      "|    2   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     4752MiB |\r\n",
      "|    3   N/A  N/A     22674      C   ...s/pytorch_p310/bin/python     4122MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "945e7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline('text-generation', \n",
    "                model=model, \n",
    "                tokenizer=tokenizer, \n",
    "                #use_cache=True,\n",
    "                #do_sample=True,\n",
    "                ##temperature = 0.2,\n",
    "                ##top_p = 0.92,\n",
    "                #top_k=5,\n",
    "                ##max_length=1000,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens=256,\n",
    "                num_return_sequences=1,\n",
    "                repetition_penalty = 1.5,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "llm_mistral = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4cf1e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AWS does not have a traditional Chief Executive Officer (CEO). Instead, it operates under an executive committee led by Andy Jassy.\n"
     ]
    }
   ],
   "source": [
    "#response = llm_mistral(\"<s>[INST] Who is the CEO of AWS? [/INST] </s>\")\n",
    "response = llm_mistral(\"Who is the CEO of AWS?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393dee2",
   "metadata": {},
   "source": [
    "##### Create QA Bot by LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24db78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af7a4875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for Q&A (1)\n",
    "prompt_template = \"\"\"<s>[INST] You are a helpful, respectful and honest assistant. Provide a concise answer based on the context.\n",
    "\n",
    "Answer the question below from context below :\n",
    "\n",
    "{context}\n",
    "\n",
    "{question}  [/INST]\"\"\"\n",
    "\n",
    "ANSWER_PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3d2622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for Q&A (2)\n",
    "prompt_template = \"\"\"Human: You are a helpful, respectful and honest assistant. Provide a concise answer based on the context.\n",
    "\n",
    "Answer the question below from context below :\n",
    "\n",
    "{context}\n",
    "\n",
    "{question}\n",
    "\n",
    "Assistant: \"\"\"\n",
    "\n",
    "ANSWER_PROMPT = PromptTemplate.from_template(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55714890",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm=llm_mistral, prompt=ANSWER_PROMPT, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40288abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does SageMaker secure my code?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7584a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search the 3 neighbor docs \n",
    "docs = vectorstore_faiss.similarity_search(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "336826eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Q: What security measures does SageMaker have?\\nSageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and\\nat rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You\\npass AWS Identity and Access Management roles to SageMaker to provide permissions to access\\nresources on your behalf for training and deployment. You can use encrypted Amazon Simple', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 1}),\n",
       " Document(page_content='downtimes. SageMaker APIs run in Amazon proven high-availability data centers, with service stack\\nreplication conﬁgured across three facilities in each Region to provide fault tolerance in the event\\nof a server failure or Availability Zone outage.\\nQ: How does SageMaker secure my code?\\nSageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted\\nat rest.\\nQ: What security measures does SageMaker have?', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 1}),\n",
       " Document(page_content='Storage Service (Amazon S3) buckets for model artifacts and data, as well as pass an AWS Key\\nManagement Service (AWS KMS) key to SageMaker notebooks, training jobs, and endpoints to\\nencrypt the attached ML storage volume. SageMaker also supports Amazon Virtual Private Cloud\\n(Amazon VPC) and AWS PrivateLink support.\\nQ: Does SageMaker use or share models, training data, or algorithms?\\nSageMaker does not use or share customer models, training data, or algorithms. We know that', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 1}),\n",
       " Document(page_content='Q: How do SageMaker Studio notebooks work?\\nSageMaker Studio notebooks are one-step Jupyter notebooks that can be spun quickly. The\\nunderlying compute resources are fully elastic, so you can easily dial up or down the available\\nresources and the changes take place automatically in the background without interrupting your\\nwork. SageMaker also enables one-step sharing of notebooks. You can easily share notebooks with\\nothers and they’ll get the exact same notebook, saved in the same place.', metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 15}),\n",
       " Document(page_content=\"Security is the top priority at AWS, and SageMaker JumpStart is designed to be secure. That's why\\nSageMaker gives you ownership and control over your content through simpliﬁed, powerful tools\\nthat help you determine where your content will be stored, secure your content in transit and at\\nrest, and manage your access to AWS services and resources for your users.\\n1. We do not share customer training and inference information with model sellers in the AWS\", metadata={'source': 'data/smdoc/Amazon SageMaker FAQs.pdf', 'page': 6})]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4ac4495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SageMaker secures user code using encryption both during transmission via SSL connections and while it is being processed within its environment. Additionally, all files uploaded into SageMaker Storage Volumes used for storing code are protected by IAM Roles assigned to them which allows fine grained permission management.\n"
     ]
    }
   ],
   "source": [
    "response = chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)[\n",
    "    \"output_text\"\n",
    "]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76fbf93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
